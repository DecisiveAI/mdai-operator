apiVersion: hub.mydecisive.ai/v1
kind: MdaiHub
metadata:
  labels:
    app.kubernetes.io/name: mdai-operator
    app.kubernetes.io/managed-by: kustomize
  name: mdaihub-second
  namespace: mdai
spec:
  variables:
    - key: service_list
      dataType: set
      serializeAs:
        - name: "SERVICE_LIST"
          transformers:
            - type: join
              join:
                delimiter: "|"
    - key: highRiskStates
      dataType: set
      serializeAs:
        - name: "HIGH_RISK_STATES"
          transformers:
            - type: join
              join:
                delimiter: "|"
    - key: filter
      dataType: string
      serializeAs:
        - name: "FILTER"
    - key: riskLevel
      dataType: string
      serializeAs:
        - name: "RISK_LEVEL"
    - key: severity_number
      dataType: int
      serializeAs:
        - name: "SEVERITY_NUMBER"
    - key: any_service_alerted
      dataType: boolean
      serializeAs:
        - name: "SERVICE_ALERTED"
    - key: attribute_map
      dataType: map
      serializeAs:
        - name: "ATTRIBUTES"
    - key: severity_filters_by_level
      dataType: map
      serializeAs:
        - name: "SEVERITY_FILTERS_BY_LEVEL"
    - key: my_priority_list
      type: meta
      dataType: metaPriorityList
      variableRefs:
        - default
        - service_list
      serializeAs:
        - name: "SERVICE_PRIORITY"
          transformers:
            - type: join
              join:
                delimiter: "|"
    - key: my_hash_set
      type: meta
      dataType: metaHashSet
      variableRefs:
        - severity_number
        - severity_filters_by_level
      serializeAs:
        - name: "SERVICE_HASH_SET"

  prometheusAlerts:
    - name: logBytesOutTooHighBySvc
      expr: 'increase(mdai_log_bytes_sent_total[1h]) > 100*1024*1024'
      severity: warning
      for: 5m
      keep_firing_for: 10m
    - name: anomalous_error_rate
      expr: 'sum(increase(error_logs_by_service_total[5m])) by (mdai_service) > 2 * sum(avg_over_time(increase(error_logs_by_service_total[5m])[1h:])) by (mdai_service) and (sum by (mdai_service) (error_logs_by_service_total offset 1h))'
      severity: warning
      for: 3m
      keep_firing_for: 3m

  rules:
    - name: HandleAddNoisyServiceToSet
      when:
        alertName: logBytesOutTooHighBySvc
        status: firing
      then:
        - addToSet:
            set: service_list
            value: "{{ service_name }}"
    - name: HandleRemoveNoisyServiceFromSet
      when:
        alertName: logBytesOutTooHighBySvc
        status: resolved
      then:
        - removeFromSet:
            set: service_list
            value: "{{ service_name }}"
    - name: TrackCriticalRiskLevel
      when:
        variableUpdated: riskLevel
        updateType: set
      then:
        - addToSet:
            set: highRiskStates
            value: "{{ variable_value }}"
    - name: anomalous_error_rate
      when:
        alertName: anomalous_error_rate
        status: firing
      then:
        - callWebhook:
            url:
              valueFrom:
                secretKeyRef: { name: slack-webhook-secret, key: url }
            templateRef: slackAlertTemplate
            templateValues:
              labels_val_ref_primary: mdai_service
              message: Service was >2x expected error rate for five minutes compared to the last hour!
              link_text: See alert in Prometheus
              link_url: http://localhost:9090/alerts